{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.3.0\n",
    "# !pip install keras\n",
    "# !pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a virtual environment actions\n",
    "def reset():\n",
    "    global P, M, It, s\n",
    "    dummy_array = np.zeros(shape=(P,8))\n",
    "    df = pd.DataFrame(dummy_array,columns=['x','y','Day','Susceptible','Exposed','Infectious','Recovered','GG'])\n",
    "    df = df.astype({'x':int,'y':int,'Day':int,'Susceptible':bool,'Exposed':int,'Infectious':int,'Recovered':bool,'GG':bool})\n",
    "    df['Susceptible'] = True\n",
    "    #Appending infectious population in \n",
    "    dfupdate=df.sample(M)\n",
    "    dfupdate['Infectious'] = np.random.randint(1,It, size=len(dfupdate))\n",
    "    dfupdate['Susceptible'] = False\n",
    "    df.update(dfupdate)\n",
    "    update_list = dfupdate.index.tolist() \n",
    "    #Dispersing people randomly among grid\n",
    "    df['x'] = np.random.randint(0,s, size=len(df))\n",
    "    df['y'] = np.random.randint(0,s, size=len(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "def update_pos(p, df):\n",
    "    global S\n",
    "    df.loc[p,'x'] = max(min(df.loc[p,'x']+random.choice(range(-1,2)),S),0) #make valid movements in the grid\n",
    "    df.loc[p,'y'] = max(min(df.loc[p,'y']+random.choice(range(-1,2)),S),0) \n",
    "    \n",
    "def coor_around(p, df):\n",
    "    return [(df.loc[p, 'x'] + a, df.loc[p, 'y'] + b) for a in range(-1,2) for b in range(-1, 2)]\n",
    "\n",
    "def one_day(df, action = 0):\n",
    "\n",
    "    # start_time = time.time()\n",
    "    global P, M, It, S, death_rate, expose_rate\n",
    "    policy_match = {0: 1, 1:0.75, 2:0.25} # assign action to policy\n",
    "    moves_under_policy = int(round(Mt * policy_match[action], 0))\n",
    "    for mt in range(moves_under_policy):\n",
    "        for p in range(len(df)):\n",
    "            if df.loc[p,'GG'] == False: #If the person is not dead\n",
    "                update_pos(p, df)\n",
    "\n",
    "                if (df.loc[p,'Infectious'] > 0) and (df.loc[p,'Recovered'] == False): #If a person is in infectious state\n",
    "                    \n",
    "                    if df.loc[p,'Infectious'] - random.choice(range(0,7)) >= It: #If the infectious days are completed\n",
    "                        if random.choice(range(0,death_rate)) > (death_rate-2): #If the person dies(with probability distribution 1:4)                           \n",
    "                            df.loc[p,'Infectious'] = 0\n",
    "                            df.loc[p,'GG'] = True #Kill the person\n",
    "                        else: #If the person survives\n",
    "                            df.loc[p,'Infectious'] = 0\n",
    "                            df.loc[p,'Recovered'] = True #Recover the person\n",
    "                    elif mt+1 == Mt:\n",
    "                        df.loc[p,'Infectious'] = df.loc[p,'Infectious'] + 1 #Increase the infectious day counter\n",
    "                        \n",
    "                elif (df.loc[p,'Exposed'] > 0) and (df.loc[p,'Infectious'] == 0): #If a person is in exposed state \n",
    "                    if (df.loc[p,'Exposed'] - random.choice(range(0,2))) >= Et: #If the person has reached the exposed day limit?  7\n",
    "                        df.loc[p,'Exposed'] = 0\n",
    "                        df.loc[p,'Infectious'] = 1 #Increase the infectious day counter, now the person is infectious\n",
    "                    elif mt+1 == Mt:\n",
    "                        df.loc[p,'Exposed'] = df.loc[p,'Exposed'] + 1 #Increase the exposed day counter\n",
    "                        \n",
    "                elif df.loc[p,'Susceptible'] == True: #If the person is in susceptible state\n",
    "                    infected_set = list(zip(df[df.Infectious > 0].x, df[df.Infectious > 0].y))\n",
    "                    if len(set(infected_set) & set(coor_around(p, df))) > 0:\n",
    "                        if random.choice(range(0,expose_rate)) > (expose_rate-2):\n",
    "                            df.loc[p,'Exposed'] = 1\n",
    "                            df.loc[p,'Susceptible'] = False\n",
    "    # print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return df # time.time() - start_time #\n",
    "\n",
    "\n",
    "def economy_gain(df):\n",
    "    economy_gain = len(df[(df.GG == False) & (df.Infectious == 0)]) * round(random.uniform(0.8,1), 2)\n",
    "    return economy_gain\n",
    "\n",
    "def current_state(df):\n",
    "    inf = len(df.loc[df['Infectious'] > 0])\n",
    "    exposed = len(df.loc[df['Exposed'] > 0]) \n",
    "    recovered = len(df.loc[df['Recovered'] == True])\n",
    "    sus = len(df.loc[df['Susceptible'] == True])\n",
    "    gg = df.loc[df['GG'] == True].GG.count()\n",
    "    \n",
    "    return np.array([recovered,sus, exposed, inf, gg])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "s = 50 #size of the grid\n",
    "N = 1000 #size of population\n",
    "M = round(N * 0.07) #Number of infectious population\n",
    "Et = 2 #Number of days staying exposed\n",
    "It = 21 #Number of days staying infectious\n",
    "Mt = 5 #Number of daily movements\n",
    "D = 200 #Number of days\n",
    "death_rate = 100\n",
    "expose_rate = 5\n",
    "\n",
    "#Initialization\n",
    "S = N - M #Susceptible population\n",
    "E = 0 #Exposed population\n",
    "I = M #Number of infectious population \n",
    "R = 0 #Recovered population\n",
    "P = S + E + I + R #Total population\n",
    "economy = 0 #Daily economic transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yi/opt/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "def_observation_space = Box(low = np.array([0,0,0,0,0]), high = np.array([P,P,P,P,P], dtype = int))\n",
    "\n",
    "# Create the virtual environment for RL\n",
    "class CoronaPolicy(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "        self.observation_space = def_observation_space # Box(low = 0, high = P, shape = (5,1), dtype = int )\n",
    "        # Dict(recovered=Discrete(P+1), sus=Discrete(P+1), exposed=Discrete(P+1),inf=Discrete(P+1),gg=Discrete(P+1))\n",
    "        \n",
    "        self.state = np.array([R, S, E, I, 0])\n",
    "        \n",
    "        self.day = 0\n",
    "        \n",
    "        self.df = reset()\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.df = one_day(self.df, action)\n",
    "        \n",
    "        self.state = current_state(self.df)\n",
    "        \n",
    "        self.day = self.day + 1\n",
    "        \n",
    "        reward = economy_gain(self.df)\n",
    "        \n",
    "        if self.day <= D:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "            \n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.observation_space = def_observation_space\n",
    "        \n",
    "        self.state = np.array([R, S, E, I, 0])\n",
    "        \n",
    "        self.day = 0\n",
    "        \n",
    "        self.df = reset()\n",
    "        \n",
    "        \n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CoronaPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 10\n",
    "# for episode in range(1, episodes+1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     economy = 0\n",
    "    \n",
    "#     while not done:\n",
    "#         action = env.action_space.sample()\n",
    "#         n_state, reward, done, info = env.step(action)\n",
    "#         economy += reward\n",
    "        \n",
    "#     print(f'Episode: {episode} Score: {economy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation = 'relu',input_shape = states))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(actions, activation = 'linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 10)                60        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 313\n",
      "Trainable params: 313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow import constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit = 1000, window_length = 1)\n",
    "    dqn = DQNAgent(model = model, memory = memory, policy = policy, \n",
    "                   nb_actions = actions, nb_steps_warmup = 10, target_model_update = 1e-2)\n",
    "                  \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   11/10000 [..............................] - ETA: 9:02:43 - reward: 824.3182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yi/opt/anaconda3/lib/python3.7/site-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000/10000 [=====>........................] - ETA: 6:44:04 - reward: 687.6062done, took 6060.772 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x118faf0d0>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr = 1e-3), metrics = ['mae'])\n",
    "dqn.fit(env, nb_steps = 2000, visualize = False, verbose = 1  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([0,100,0,1,0])\n",
    "# # a = tf.constant([1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
    "# # print(a)\n",
    "# a=tf.reshape(a, [1, 5]) \n",
    "# # print(c)\n",
    "# model.predict(a, steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-ef34982af1db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episode_reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yi/opt/anaconda3/lib/python3.7/site-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_repetition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-152-16dabf22df09>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-e53e85a091ac>\u001b[0m in \u001b[0;36mone_day\u001b[0;34m(df, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'GG'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#If the person is not dead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mupdate_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Infectious'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Recovered'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#If a person is in infectious state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-e53e85a091ac>\u001b[0m in \u001b[0;36mupdate_pos\u001b[0;34m(p, df)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#make valid movements in the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yi/.local/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yi/.local/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    618\u001b[0m                 \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                     \u001b[0msetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yi/.local/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36msetter\u001b[0;34m(item, v)\u001b[0m\n\u001b[1;32m    536\u001b[0m                     \u001b[0;31m# set the item, possibly having a dtype change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yi/.local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   5802\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5803\u001b[0m         \"\"\"\n\u001b[0;32m-> 5804\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5805\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yi/.local/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         return self.apply('copy', axes=new_axes, deep=deep,\n\u001b[0;32m--> 734\u001b[0;31m                           do_integrity_check=False)\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yi/.local/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                                             copy=align_copy)\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yi/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     def replace(self, to_replace, value, inplace=False, filter=None,\n",
      "\u001b[0;32m/Users/yi/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block_same_class\u001b[0;34m(self, values, placement, ndim, dtype)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mplacement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         return make_block(values, placement=placement, ndim=ndim,\n\u001b[0;32m--> 235\u001b[0;31m                           klass=self.__class__, dtype=dtype)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__unicode__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yi/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m def make_block(values, placement, klass=None, ndim=None, dtype=None,\n\u001b[0m\u001b[1;32m   3081\u001b[0m                fastpath=None):\n\u001b[1;32m   3082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfastpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes = 1, visualize = False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_ann_4/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model_ann_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The implementation of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0: take action 2, total_reward: 855.6. [[ 8665.851  -1057.5657  8819.136 ]]\n",
      "Day 1: take action 2, total_reward: 1646.1. [[ 8517.313  -1049.3373  8661.883 ]]\n",
      "Day 2: take action 2, total_reward: 2511.0. [[ 8347.556  -1039.9342  8482.166 ]]\n",
      "Day 3: take action 2, total_reward: 3348.0. [[ 8186.288  -1031.0011  8311.4375]]\n",
      "Day 4: take action 2, total_reward: 4092.0. [[ 8067.4575 -1024.4187  8185.6343]]\n",
      "Day 5: take action 2, total_reward: 4919.7. [[ 7961.3604  -1018.54205  8073.313  ]]\n",
      "Day 6: take action 2, total_reward: 5803.2. [[ 7846.7734  -1012.19446  7952.0024 ]]\n",
      "Day 7: take action 2, total_reward: 6723.9. [[ 7719.4556 -1005.1419  7817.2153]]\n",
      "Day 8: take action 2, total_reward: 7551.599999999999. [[7600.627   -998.55994 7691.4146 ]]\n",
      "Day 9: take action 2, total_reward: 8407.199999999999. [[7507.2324 -993.8667 7592.2334]]\n",
      "Day 10: take action 2, total_reward: 9318.599999999999. [[7466.2285 -994.2193 7547.144 ]]\n",
      "Day 11: take action 2, total_reward: 10136.999999999998. [[7433.109  -994.5034 7510.7256]]\n",
      "Day 12: take action 2, total_reward: 10964.699999999999. [[7415.761   -994.65204 7491.6494 ]]\n",
      "Day 13: take action 2, total_reward: 11773.8. [[7396.835   -994.81445 7470.8384 ]]\n",
      "Day 14: take action 2, total_reward: 12675.9. [[7376.333   -994.99084 7448.2935 ]]\n",
      "Day 15: take action 2, total_reward: 13512.9. [[7357.4067 -995.1528 7427.4824]]\n",
      "Day 16: take action 2, total_reward: 14331.3. [[7327.4424 -995.4101 7394.533 ]]\n",
      "Day 17: take action 2, total_reward: 15177.599999999999. [[7306.94   -995.5863 7371.9883]]\n",
      "Day 18: take action 2, total_reward: 16079.699999999999. [[7291.169   -995.72125 7354.6455 ]]\n",
      "Day 19: take action 2, total_reward: 16944.6. [[7275.398   -995.85693 7337.304  ]]\n",
      "Day 20: take action 2, total_reward: 17707.199999999997. [[7259.6265  -995.99243 7319.9614 ]]\n",
      "Day 21: take action 2, total_reward: 18572.1. [[7237.5474  -996.18176 7295.6836 ]]\n",
      "Day 22: take action 2, total_reward: 19381.199999999997. [[7215.468  -996.3716 7271.4043]]\n",
      "Day 23: take action 2, total_reward: 20283.299999999996. [[7198.119  -996.5203 7252.327 ]]\n",
      "Day 24: take action 2, total_reward: 21120.299999999996. [[7179.195   -996.68256 7231.517  ]]\n",
      "Day 25: take action 2, total_reward: 22040.999999999996. [[7160.2695 -996.8451 7210.7065]]\n",
      "Day 26: take action 2, total_reward: 22924.499999999996. [[7144.4976  -996.98016 7193.3647 ]]\n",
      "Day 27: take action 2, total_reward: 23798.699999999997. [[7171.6875  -998.62714 7219.028  ]]\n",
      "Day 28: take action 2, total_reward: 24691.499999999996. [[ 7214.281  -1000.9242  7259.8643]]\n",
      "Day 29: take action 2, total_reward: 25547.099999999995. [[ 7250.322  -1002.8683  7294.4175]]\n",
      "Day 30: take action 2, total_reward: 26300.399999999994. [[ 7289.6387 -1004.989   7332.1133]]\n",
      "Day 31: take action 2, total_reward: 27072.299999999996. [[ 7299.4683 -1005.5193  7341.5376]]\n",
      "Day 32: take action 2, total_reward: 27825.599999999995. [[ 7335.509  -1007.4631  7376.0913]]\n",
      "Day 33: take action 2, total_reward: 28634.699999999993. [[ 7364.998  -1009.0545  7404.363 ]]\n",
      "Day 34: take action 2, total_reward: 29546.099999999995. [[ 7381.379  -1009.9374  7420.069 ]]\n",
      "Day 35: take action 2, total_reward: 30355.199999999993. [[ 7394.485  -1010.6443  7432.635 ]]\n",
      "Day 36: take action 2, total_reward: 31164.299999999992. [[ 7420.6963  -1012.05786  7457.765  ]]\n",
      "Day 37: take action 2, total_reward: 31991.999999999993. [[ 7460.0137 -1014.1791  7495.4604]]\n",
      "Day 38: take action 2, total_reward: 32838.299999999996. [[ 7486.2246  -1015.59283  7520.589  ]]\n",
      "Day 39: take action 2, total_reward: 33656.7. [[ 7512.4365  -1017.00696  7545.72   ]]\n",
      "Day 40: take action 2, total_reward: 34512.299999999996. [[ 7532.0947 -1018.0668  7564.568 ]]\n",
      "Day 41: take action 2, total_reward: 35423.7. [[ 7548.4766 -1018.9507  7580.274 ]]\n",
      "Day 42: take action 2, total_reward: 36223.5. [[ 7558.3066  -1019.48114  7589.698  ]]\n",
      "Day 43: take action 2, total_reward: 37051.2. [[ 7571.413   -1020.18823  7602.264  ]]\n",
      "Day 44: take action 2, total_reward: 37953.299999999996. [[ 7587.795   -1021.07117  7617.97   ]]\n",
      "Day 45: take action 2, total_reward: 38873.99999999999. [[ 7600.9    -1021.7783  7630.534 ]]\n",
      "Day 46: take action 2, total_reward: 39794.69999999999. [[ 7623.836  -1023.0157  7652.524 ]]\n",
      "Day 47: take action 2, total_reward: 40603.79999999999. [[ 7646.7705 -1024.253   7674.5127]]\n",
      "Day 48: take action 2, total_reward: 41422.19999999999. [[ 7666.429  -1025.3126  7693.361 ]]\n",
      "Day 49: take action 2, total_reward: 42249.89999999999. [[ 7686.0874 -1026.3737  7712.2075]]\n",
      "Day 50: take action 2, total_reward: 43142.69999999999. [[ 7695.917  -1026.9036  7721.632 ]]\n",
      "Day 51: take action 2, total_reward: 44044.79999999999. [[ 7699.194  -1027.0803  7724.774 ]]\n",
      "Day 52: take action 2, total_reward: 44909.69999999999. [[ 7709.023  -1027.6104  7734.1973]]\n",
      "Day 53: take action 2, total_reward: 45839.69999999999. [[ 7725.4062 -1028.4944  7749.9043]]\n",
      "Day 54: take action 2, total_reward: 46741.79999999999. [[ 7735.2354 -1029.0243  7759.3286]]\n",
      "Day 55: take action 2, total_reward: 47513.69999999999. [[ 7751.617 -1029.908  7775.034]]\n",
      "Day 56: take action 2, total_reward: 48304.19999999999. [[ 7764.722  -1030.6149  7787.5986]]\n",
      "Day 57: take action 2, total_reward: 49085.39999999999. [[ 7771.275  -1030.9684  7793.8813]]\n",
      "Day 58: take action 2, total_reward: 49894.499999999985. [[ 7787.657  -1031.8518  7809.588 ]]\n",
      "Day 59: take action 2, total_reward: 50824.499999999985. [[ 7804.04   -1032.7356  7825.295 ]]\n",
      "Day 60: take action 2, total_reward: 51745.19999999998. [[ 7810.5923 -1033.0891  7831.577 ]]\n",
      "Day 61: take action 2, total_reward: 52637.999999999985. [[ 7830.25   -1034.1492  7850.424 ]]\n",
      "Day 62: take action 2, total_reward: 53521.499999999985. [[ 7843.356  -1034.8563  7862.9893]]\n",
      "Day 63: take action 2, total_reward: 54321.29999999999. [[ 7856.462 -1035.563  7875.554]]\n",
      "Day 64: take action 2, total_reward: 55093.19999999999. [[ 7866.292  -1036.0938  7884.979 ]]\n",
      "Day 65: take action 2, total_reward: 55967.39999999999. [[ 7872.845  -1036.4469  7891.262 ]]\n",
      "Day 66: take action 2, total_reward: 56720.69999999999. [[ 7872.845  -1036.4469  7891.262 ]]\n",
      "Day 67: take action 2, total_reward: 57594.89999999999. [[ 7885.9507 -1037.154   7903.826 ]]\n",
      "Day 68: take action 2, total_reward: 58496.999999999985. [[ 7892.504  -1037.5072  7910.11  ]]\n",
      "Day 69: take action 2, total_reward: 59408.39999999999. [[ 7918.7144 -1038.921   7935.2393]]\n",
      "Day 70: take action 2, total_reward: 60263.999999999985. [[ 7925.268  -1039.2748  7941.5215]]\n",
      "Day 71: take action 2, total_reward: 61156.79999999999. [[ 7938.373  -1039.9814  7954.087 ]]\n",
      "Day 72: take action 2, total_reward: 62077.499999999985. [[ 7944.9272 -1040.3348  7960.37  ]]\n",
      "Day 73: take action 2, total_reward: 62895.89999999999. [[ 7958.0327 -1041.0422  7972.9355]]\n",
      "Day 74: take action 2, total_reward: 63788.69999999999. [[ 7967.863 -1041.572  7982.36 ]]\n",
      "Day 75: take action 2, total_reward: 64588.49999999999. [[ 7967.863 -1041.572  7982.36 ]]\n",
      "Day 76: take action 2, total_reward: 65462.69999999999. [[ 7980.968  -1042.2786  7994.9243]]\n",
      "Day 77: take action 2, total_reward: 66281.09999999999. [[ 7984.2437 -1042.4556  7998.066 ]]\n",
      "Day 78: take action 2, total_reward: 67034.4. [[ 8000.626  -1043.3395  8013.7715]]\n",
      "Day 79: take action 2, total_reward: 67917.9. [[ 8017.0073 -1044.2231  8029.4775]]\n",
      "Day 80: take action 2, total_reward: 68661.9. [[ 8026.837 -1044.753  8038.902]]\n",
      "Day 81: take action 2, total_reward: 69489.59999999999. [[ 8030.1123 -1044.9298  8042.042 ]]\n",
      "Day 82: take action 2, total_reward: 70270.79999999999. [[ 8039.944  -1045.4602  8051.4683]]\n",
      "Day 83: take action 2, total_reward: 71051.99999999999. [[ 8053.0483 -1046.1669  8064.032 ]]\n",
      "Day 84: take action 2, total_reward: 71963.39999999998. [[ 8059.602  -1046.5206  8070.3154]]\n",
      "Day 85: take action 2, total_reward: 72828.29999999997. [[ 8072.707  -1047.2275  8082.8794]]\n",
      "Day 86: take action 2, total_reward: 73628.09999999998. [[ 8089.09   -1048.1108  8098.5864]]\n",
      "Day 87: take action 2, total_reward: 74558.09999999998. [[ 8095.642  -1048.4644  8104.8687]]\n",
      "Day 88: take action 2, total_reward: 75422.99999999997. [[ 8102.1953 -1048.8177  8111.151 ]]\n",
      "Day 89: take action 2, total_reward: 76241.39999999997. [[ 8108.7476 -1049.1713  8117.4326]]\n",
      "Day 90: take action 2, total_reward: 77124.89999999997. [[ 8112.0244 -1049.3481  8120.575 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 91: take action 2, total_reward: 77980.49999999997. [[ 8115.302  -1049.5249  8123.7163]]\n",
      "Day 92: take action 2, total_reward: 78789.59999999998. [[ 8125.13   -1050.0549  8133.14  ]]\n",
      "Day 93: take action 2, total_reward: 79719.59999999998. [[ 8128.4067 -1050.2318  8136.2812]]\n",
      "Day 94: take action 2, total_reward: 80537.99999999997. [[ 8131.6836 -1050.4083  8139.423 ]]\n",
      "Day 95: take action 2, total_reward: 81281.99999999997. [[ 8134.959  -1050.5853  8142.564 ]]\n",
      "Day 96: take action 2, total_reward: 82193.39999999997. [[ 8134.959  -1050.5853  8142.564 ]]\n",
      "Day 97: take action 2, total_reward: 83011.79999999996. [[ 8134.959  -1050.5853  8142.564 ]]\n",
      "Day 98: take action 2, total_reward: 83792.99999999996. [[ 8148.0654 -1051.292   8155.129 ]]\n",
      "Day 99: take action 2, total_reward: 84546.29999999996. [[ 8148.0654 -1051.292   8155.129 ]]\n",
      "Day 100: take action 2, total_reward: 85346.09999999996. [[ 8157.8955 -1051.8226  8164.553 ]]\n",
      "Day 101: take action 2, total_reward: 86117.99999999996. [[ 8157.8955 -1051.8226  8164.553 ]]\n",
      "Day 102: take action 2, total_reward: 87029.39999999995. [[ 8157.8955 -1051.8226  8164.553 ]]\n",
      "Day 103: take action 2, total_reward: 87829.19999999995. [[ 8161.171  -1051.9991  8167.695 ]]\n",
      "Day 104: take action 2, total_reward: 88582.49999999996. [[ 8167.7236 -1052.3525  8173.9766]]\n",
      "Day 105: take action 2, total_reward: 89475.29999999996. [[ 8171.001  -1052.5293  8177.118 ]]\n",
      "Day 106: take action 2, total_reward: 90377.39999999997. [[ 8177.5537 -1052.8828  8183.401 ]]\n",
      "Day 107: take action 2, total_reward: 91167.89999999997. [[ 8177.5537 -1052.8828  8183.401 ]]\n",
      "Day 108: take action 2, total_reward: 91949.09999999996. [[ 8180.8296 -1053.0597  8186.5415]]\n",
      "Day 109: take action 2, total_reward: 92860.49999999996. [[ 8184.1064 -1053.2365  8189.6826]]\n",
      "Day 110: take action 2, total_reward: 93725.39999999995. [[ 8190.6587 -1053.5894  8195.966 ]]\n",
      "Day 111: take action 2, total_reward: 94562.39999999995. [[ 8200.488  -1054.1199  8205.39  ]]\n",
      "Day 112: take action 2, total_reward: 95492.39999999995. [[ 8203.765  -1054.2966  8208.53  ]]\n",
      "Day 113: take action 2, total_reward: 96357.29999999994. [[ 8213.594 -1054.827  8217.955]]\n",
      "Day 114: take action 2, total_reward: 97129.19999999994. [[ 8213.594 -1054.827  8217.955]]\n",
      "Day 115: take action 2, total_reward: 97975.49999999994. [[ 8216.871  -1055.0034  8221.097 ]]\n",
      "Day 116: take action 2, total_reward: 98803.19999999994. [[ 8220.146  -1055.1802  8224.237 ]]\n",
      "Day 117: take action 2, total_reward: 99565.79999999994. [[ 8226.7    -1055.5337  8230.5205]]\n",
      "Day 118: take action 2, total_reward: 100495.79999999994. [[ 8226.7    -1055.5337  8230.5205]]\n",
      "Day 119: take action 2, total_reward: 101276.99999999994. [[ 8229.977  -1055.7106  8233.661 ]]\n",
      "Day 120: take action 2, total_reward: 102197.69999999994. [[ 8233.252  -1055.8873  8236.802 ]]\n",
      "Day 121: take action 2, total_reward: 103016.09999999993. [[ 8233.252  -1055.8873  8236.802 ]]\n",
      "Day 122: take action 2, total_reward: 103797.29999999993. [[ 8239.805  -1056.2406  8243.086 ]]\n",
      "Day 123: take action 2, total_reward: 104587.79999999993. [[ 8239.805  -1056.2406  8243.086 ]]\n",
      "Day 124: take action 2, total_reward: 105368.99999999993. [[ 8246.358  -1056.5938  8249.368 ]]\n",
      "Day 125: take action 2, total_reward: 106243.19999999992. [[ 8252.912  -1056.9475  8255.651 ]]\n",
      "Day 126: take action 2, total_reward: 107154.59999999992. [[ 8256.1875 -1057.1239  8258.792 ]]\n",
      "Day 127: take action 2, total_reward: 107907.89999999992. [[ 8256.1875 -1057.1239  8258.792 ]]\n",
      "Day 128: take action 2, total_reward: 108716.99999999993. [[ 8259.465  -1057.3011  8261.934 ]]\n",
      "Day 129: take action 2, total_reward: 109479.59999999993. [[ 8266.018  -1057.6549  8268.215 ]]\n",
      "Day 130: take action 2, total_reward: 110297.99999999993. [[ 8266.018  -1057.6549  8268.215 ]]\n",
      "Day 131: take action 2, total_reward: 111172.19999999992. [[ 8266.018  -1057.6549  8268.215 ]]\n",
      "Day 132: take action 2, total_reward: 112037.09999999992. [[ 8266.018  -1057.6549  8268.215 ]]\n",
      "Day 133: take action 2, total_reward: 112948.49999999991. [[ 8269.294 -1057.831  8271.356]]\n",
      "Day 134: take action 2, total_reward: 113804.09999999992. [[ 8269.294 -1057.831  8271.356]]\n",
      "Day 135: take action 2, total_reward: 114641.09999999992. [[ 8269.294 -1057.831  8271.356]]\n",
      "Day 136: take action 2, total_reward: 115487.39999999992. [[ 8272.57   -1058.0078  8274.498 ]]\n",
      "Day 137: take action 2, total_reward: 116380.19999999992. [[ 8272.57   -1058.0078  8274.498 ]]\n",
      "Day 138: take action 2, total_reward: 117300.89999999992. [[ 8272.57   -1058.0078  8274.498 ]]\n",
      "Day 139: take action 2, total_reward: 118221.59999999992. [[ 8275.848  -1058.1848  8277.641 ]]\n",
      "Day 140: take action 2, total_reward: 119114.39999999992. [[ 8275.848  -1058.1848  8277.641 ]]\n",
      "Day 141: take action 2, total_reward: 119923.49999999993. [[ 8279.123  -1058.3616  8280.78  ]]\n",
      "Day 142: take action 2, total_reward: 120751.19999999992. [[ 8282.399  -1058.5385  8283.922 ]]\n",
      "Day 143: take action 2, total_reward: 121569.59999999992. [[ 8282.399  -1058.5385  8283.922 ]]\n",
      "Day 144: take action 2, total_reward: 122378.69999999992. [[ 8285.677  -1058.715   8287.0625]]\n",
      "Day 145: take action 2, total_reward: 123159.89999999992. [[ 8285.677  -1058.715   8287.0625]]\n",
      "Day 146: take action 2, total_reward: 124089.89999999992. [[ 8288.953  -1058.8917  8290.205 ]]\n",
      "Day 147: take action 2, total_reward: 124861.79999999992. [[ 8288.953  -1058.8917  8290.205 ]]\n",
      "Day 148: take action 2, total_reward: 125615.09999999992. [[ 8292.228  -1059.0681  8293.345 ]]\n",
      "Day 149: take action 2, total_reward: 126535.79999999992. [[ 8298.781  -1059.4216  8299.628 ]]\n",
      "Day 150: take action 2, total_reward: 127372.79999999992. [[ 8298.781  -1059.4216  8299.628 ]]\n",
      "Day 151: take action 2, total_reward: 128191.19999999991. [[ 8298.781  -1059.4216  8299.628 ]]\n",
      "Day 152: take action 2, total_reward: 129083.99999999991. [[ 8305.335  -1059.7751  8305.91  ]]\n",
      "Day 153: take action 2, total_reward: 129827.99999999991. [[ 8315.164  -1060.3053  8315.335 ]]\n",
      "Day 154: take action 2, total_reward: 130581.29999999992. [[ 8315.164  -1060.3053  8315.335 ]]\n",
      "Day 155: take action 0, total_reward: 131408.9999999999. [[ 8321.717  -1060.6587  8321.618 ]]\n",
      "Day 156: take action 0, total_reward: 131501.06999999992. [[ 8324.992  -1060.8352  8324.758 ]]\n",
      "Day 157: take action 0, total_reward: 131541.1199999999. [[2402.9546 -598.1659 2227.5647]]\n",
      "Day 158: take action 0, total_reward: 131564.3399999999. [[2352.3994 -585.4173 2180.8423]]\n",
      "Day 159: take action 0, total_reward: 131583.03999999992. [[2389.961   -581.58887 2227.92   ]]\n",
      "Day 160: take action 0, total_reward: 131602.3999999999. [[2419.7988  -580.71515 2263.302  ]]\n",
      "Day 161: take action 0, total_reward: 131618.59999999992. [[2454.5278 -580.9443 2303.326 ]]\n",
      "Day 162: take action 0, total_reward: 131638.3999999999. [[2460.4146  -580.51984 2310.541  ]]\n",
      "Day 163: take action 0, total_reward: 131662.7599999999. [[2497.1     -581.18994 2352.4219 ]]\n",
      "Day 164: take action 0, total_reward: 131692.8299999999. [[2592.1167 -583.4485 2460.4087]]\n",
      "Day 165: take action 0, total_reward: 131720.8799999999. [[2620.0625 -584.1991 2492.0886]]\n"
     ]
    }
   ],
   "source": [
    "df = reset()\n",
    "economy = 0\n",
    "\n",
    "for day in range(0, D+1):\n",
    "    state = current_state(df)\n",
    "    state =tf.reshape(state, [1, 5])\n",
    "    prediction = model.predict(state, steps = 1)\n",
    "    action_by_agent = np.argmax(prediction)\n",
    "    df = one_day(df, action = action_by_agent)\n",
    "    gain = economy_gain(df)\n",
    "    economy += gain\n",
    "    print(f\"Day {day}: take action {action_by_agent}, total_reward: {economy}. {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-73895838759f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "(max(max(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8665.851 , -1057.5657,  8819.136 ], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
