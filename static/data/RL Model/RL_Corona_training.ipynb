{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.3.0\n",
    "# !pip install keras\n",
    "# !pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a virtual environment actions\n",
    "def reset():\n",
    "    global P, M, It, s\n",
    "    dummy_array = np.zeros(shape=(P,8))\n",
    "    df = pd.DataFrame(dummy_array,columns=['x','y','Day','Susceptible','Exposed','Infectious','Recovered','GG'])\n",
    "    df = df.astype({'x':int,'y':int,'Day':int,'Susceptible':bool,'Exposed':int,'Infectious':int,'Recovered':bool,'GG':bool})\n",
    "    df['Susceptible'] = True\n",
    "    #Appending infectious population in \n",
    "    dfupdate=df.sample(M)\n",
    "    dfupdate['Infectious'] = np.random.randint(1,It, size=len(dfupdate))\n",
    "    dfupdate['Susceptible'] = False\n",
    "    df.update(dfupdate)\n",
    "    update_list = dfupdate.index.tolist() \n",
    "    #Dispersing people randomly among grid\n",
    "    df['x'] = np.random.randint(0,s, size=len(df))\n",
    "    df['y'] = np.random.randint(0,s, size=len(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def coor_around(p, df):\n",
    "    return [(df.loc[p, 'x'] + a, df.loc[p, 'y'] + b) for a in range(-1,2) for b in range(-1, 2)]\n",
    "\n",
    "def one_day(df, action = 0):\n",
    "\n",
    "    # start_time = time.time()\n",
    "    global P, M, It, S, death_rate, expose_rate\n",
    "    policy_match = {0: 1, 1:0.75, 2:0.25} # assign action to policy\n",
    "\n",
    "    df_infectious = df.loc[(df['Infectious'] > 0)]\n",
    "    df_infectious = df_infectious[['x', 'y']]\n",
    "\n",
    "    moves_under_policy = int(round(Mt * policy_match[action], 0))\n",
    "    for mt in range(moves_under_policy):\n",
    "        for index, person in df.iterrows():\n",
    "\n",
    "            if not person['GG']:  # If the person is not dead\n",
    "\n",
    "                new_move_x = random.choice(range(-1, 2))\n",
    "                new_move_y = random.choice(range(-1, 2))\n",
    "\n",
    "                person['x'] = max(min(person['x'] + new_move_x, s), 0)\n",
    "                person['y'] = max(min(person['y'] + new_move_y, s), 0)\n",
    "\n",
    "                df.iat[index, 0] = int(person['x'])\n",
    "                df.iat[index, 1] = int(person['y'])\n",
    "\n",
    "                if index in df_infectious.index:  # assigning whats in person (row) to df_infectious at the correct index\n",
    "                    df_infectious.at[index, 'x'] = person['x']\n",
    "                    df_infectious.at[index, 'y'] = person['y']\n",
    "\n",
    "                df.at[index, 'Day'] = day + 1  # updating the day counter\n",
    "\n",
    "                if (person['Infectious'] > 0) and (person['Recovered'] == False):  # If a person is in infectious state\n",
    "                    if person['Infectious'] - random.choice(range(0, 7)) >= It:  # If the infectious days are completed\n",
    "                        if random.choice(range(0, death_rate)) > (\n",
    "                                death_rate - 2):  # If the person dies(with probability distribution 1:4)\n",
    "                            df.at[index, 'Infectious'] = 0\n",
    "                            if index in df_infectious.index:\n",
    "                                df_infectious.drop([index])\n",
    "\n",
    "                            df.at[index, 'GG'] = True  # Kill the person\n",
    "                        else:  # If the person survives\n",
    "                            df.at[index, 'Infectious'] = 0\n",
    "                            if index in df_infectious.index:\n",
    "                                df_infectious.drop([index])\n",
    "                            df.at[index, 'Recovered'] = True  # Recover the person\n",
    "                    elif mt + 1 == Mt:\n",
    "                        df.at[index, 'Infectious'] = person['Infectious'] + 1  # Increase the infectious day counter\n",
    "                elif (person['Exposed'] > 0) and (person['Infectious'] == 0):  # If a person is in exposed state\n",
    "                    if (person['Exposed'] - random.choice(\n",
    "                            range(0, 2))) >= Et:  # If the person has reached the exposed day limit?  7\n",
    "                        df.at[index, 'Exposed'] = 0\n",
    "                        df.at[\n",
    "                            index, 'Infectious'] = 1  # Increase the infectious day counter, now the person is infectious\n",
    "                        df_infectious.append(person)\n",
    "                    elif mt + 1 == Mt:\n",
    "                        df.at[index, 'Exposed'] = person['Exposed'] + 1  # Increase the exposed day counter\n",
    "\n",
    "                elif person['Susceptible']:  # If the person is in susceptible state\n",
    "\n",
    "                    x_temp = int(person['x'])\n",
    "                    df_xtemp = df_infectious[['x']].to_numpy()\n",
    "\n",
    "                    if (x_temp in df_xtemp)or ((x_temp - 1) in df_xtemp) or ((x_temp + 1) in df_xtemp):\n",
    "\n",
    "                        y_temp = int(person['y'])\n",
    "                        df_ytemp = df_infectious[['y']].to_numpy()\n",
    "                        if (y_temp in df_ytemp) or ((y_temp - 1) in df_ytemp) or ((y_temp + 1) in df_ytemp):\n",
    "                            if random.choice(range(0, expose_rate)) > (expose_rate - 2):\n",
    "                                df.at[index, 'Exposed'] = 1\n",
    "                                df.at[index, 'Susceptible'] = False\n",
    "    # print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return df # time.time() - start_time #\n",
    "\n",
    "\n",
    "def economy_gain(df):\n",
    "    economy_gain = len(df[(df.GG == False) & (df.Infectious == 0)]) * round(random.uniform(0.8,1), 2)\n",
    "    return economy_gain\n",
    "\n",
    "def current_state(df):\n",
    "    inf = len(df.loc[df['Infectious'] > 0])\n",
    "    exposed = len(df.loc[df['Exposed'] > 0]) \n",
    "    recovered = len(df.loc[df['Recovered'] == True])\n",
    "    sus = len(df.loc[df['Susceptible'] == True])\n",
    "    gg = df.loc[df['GG'] == True].GG.count()\n",
    "    \n",
    "    return np.array([recovered,sus, exposed, inf, gg])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "s = 50 #size of the grid\n",
    "N = 1000 #size of population\n",
    "M = round(N * 0.07) #Number of infectious population\n",
    "Et = 2 #Number of days staying exposed\n",
    "It = 21 #Number of days staying infectious\n",
    "Mt = 5 #Number of daily movements\n",
    "D = 200 #Number of days\n",
    "death_rate = 100\n",
    "expose_rate = 5\n",
    "\n",
    "#Initialization\n",
    "S = N - M #Susceptible population\n",
    "E = 0 #Exposed population\n",
    "I = M #Number of infectious population \n",
    "R = 0 #Recovered population\n",
    "P = S + E + I + R #Total population\n",
    "economy = 0 #Daily economic transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_observation_space = Box(low = np.array([0,0,0,0,0]), high = np.array([P,P,P,P,P], dtype = int))\n",
    "\n",
    "# Create the virtual environment for RL\n",
    "class CoronaPolicy(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "        self.observation_space = def_observation_space # Box(low = 0, high = P, shape = (5,1), dtype = int )\n",
    "        # Dict(recovered=Discrete(P+1), sus=Discrete(P+1), exposed=Discrete(P+1),inf=Discrete(P+1),gg=Discrete(P+1))\n",
    "        \n",
    "        self.state = np.array([R, S, E, I, 0])\n",
    "        \n",
    "        self.day = 0\n",
    "        \n",
    "        self.df = reset()\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.df = one_day(self.df, action)\n",
    "        \n",
    "        self.state = current_state(self.df)\n",
    "        \n",
    "        self.day = self.day + 1\n",
    "        \n",
    "        reward = economy_gain(self.df)\n",
    "        \n",
    "        if self.day <= D:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "            \n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.observation_space = def_observation_space\n",
    "        \n",
    "        self.state = np.array([R, S, E, I, 0])\n",
    "        \n",
    "        self.day = 0\n",
    "        \n",
    "        self.df = reset()\n",
    "        \n",
    "        \n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CoronaPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 10\n",
    "# for episode in range(1, episodes+1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     economy = 0\n",
    "    \n",
    "#     while not done:\n",
    "#         action = env.action_space.sample()\n",
    "#         n_state, reward, done, info = env.step(action)\n",
    "#         economy += reward\n",
    "        \n",
    "#     print(f'Episode: {episode} Score: {economy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation = 'relu',input_shape = states))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(actions, activation = 'linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                60        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 313\n",
      "Trainable params: 313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-16-a3107deafc58>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mrl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0magents\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDQNAgent\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mrl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolicy\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mBoltzmannQPolicy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mrl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmemory\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSequentialMemory\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mconstant\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'rl'"
     ]
    }
   ],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow import constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit = 1000, window_length = 1)\n",
    "    dqn = DQNAgent(model = model, memory = memory, policy = policy, \n",
    "                   nb_actions = actions, nb_steps_warmup = 10, target_model_update = 1e-2)\n",
    "                  \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr = 1e-3), metrics = ['mae'])\n",
    "dqn.fit(env, nb_steps = 2000, visualize = False, verbose = 1  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([0,100,0,1,0])\n",
    "# # a = tf.constant([1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
    "# # print(a)\n",
    "# a=tf.reshape(a, [1, 5]) \n",
    "# # print(c)\n",
    "# model.predict(a, steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes = 1, visualize = False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_ann_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The implementation of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reset()\n",
    "economy = 0\n",
    "\n",
    "for day in range(0, D+1):\n",
    "    state = current_state(df)\n",
    "    state = tf.reshape(state, [1, 5])\n",
    "    prediction = model.predict(state, steps = 1)\n",
    "    action_by_agent = np.argmax(prediction)\n",
    "    df = one_day(df, action = action_by_agent)\n",
    "    gain = economy_gain(df)\n",
    "    economy += gain\n",
    "    print(f\"Day {day}: take action {action_by_agent}, total_reward: {economy}. {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(max(max(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}