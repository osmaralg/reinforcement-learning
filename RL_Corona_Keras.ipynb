{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.3.0\n",
    "# !pip install keras\n",
    "# !pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a virtual environment actions\n",
    "def reset():\n",
    "    global P, M, It, s\n",
    "    dummy_array = np.zeros(shape=(P,8))\n",
    "    df = pd.DataFrame(dummy_array,columns=['x','y','Day','Susceptible','Exposed','Infectious','Recovered','GG'])\n",
    "    df = df.astype({'x':int,'y':int,'Day':int,'Susceptible':bool,'Exposed':int,'Infectious':int,'Recovered':bool,'GG':bool})\n",
    "    df['Susceptible'] = True\n",
    "    #Appending infectious population in \n",
    "    dfupdate=df.sample(M)\n",
    "    dfupdate['Infectious'] = np.random.randint(1,It, size=len(dfupdate))\n",
    "    dfupdate['Susceptible'] = False\n",
    "    df.update(dfupdate)\n",
    "    update_list = dfupdate.index.tolist() \n",
    "    #Dispersing people randomly among grid\n",
    "    df['x'] = np.random.randint(0,s, size=len(df))\n",
    "    df['y'] = np.random.randint(0,s, size=len(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "def update_pos(p, df):\n",
    "    global S\n",
    "    df.loc[p,'x'] = max(min(df.loc[p,'x']+random.choice(range(-1,2)),s),0) #make valid movements in the grid\n",
    "    df.loc[p,'y'] = max(min(df.loc[p,'y']+random.choice(range(-1,2)),s),0) \n",
    "    \n",
    "def coor_around(p, df):\n",
    "    return [(df.loc[p, 'x'] + a, df.loc[p, 'y'] + b) for a in range(-1,2) for b in range(-1, 2)]\n",
    "\n",
    "def one_day(df, action=0):\n",
    "    # start_time = time.time()\n",
    "    global P, M, It, S, death_rate, expose_rate\n",
    "    policy_match = {0: 1, 1: 0.75, 2: 0.25}  # assign action to policy\n",
    "    moves_under_policy = int(round(Mt * policy_match[action], 0))\n",
    "\n",
    "    df_infectious = df.loc[(df['Infectious'] > 0)]\n",
    "    df_infectious = df_infectious[['x', 'y']]\n",
    "\n",
    "    for mt in range(moves_under_policy):\n",
    "        for index, person in df.iterrows():\n",
    "\n",
    "            if not person['GG']:  # If the person is not dead\n",
    "\n",
    "                new_move_x = random.choice(range(-1, 2))\n",
    "                new_move_y = random.choice(range(-1, 2))\n",
    "\n",
    "                person['x'] = max(min(person['x'] + new_move_x, s), 0)\n",
    "                person['y'] = max(min(person['y'] + new_move_y, s), 0)\n",
    "\n",
    "                df.iat[index, 0] = int(person['x'])\n",
    "                df.iat[index, 1] = int(person['y'])\n",
    "\n",
    "                if index in df_infectious.index:  # assigning whats in person (row) to df_infectious at the correct index\n",
    "                    df_infectious.at[index, 'x'] = person['x']\n",
    "                    df_infectious.at[index, 'y'] = person['y']\n",
    "\n",
    "                if (person['Infectious'] > 0) and (person['Recovered'] == False):  # If a person is in infectious state\n",
    "                    if person['Infectious'] - random.choice(range(0, 7)) >= It:  # If the infectious days are completed\n",
    "                        if random.choice(range(0, death_rate)) > (\n",
    "                                death_rate - 2):  # If the person dies(with probability distribution 1:4)\n",
    "                            df.at[index, 'Infectious'] = 0\n",
    "                            if index in df_infectious.index:\n",
    "                                df_infectious.drop([index])\n",
    "\n",
    "                            df.at[index, 'GG'] = True  # Kill the person\n",
    "                        else:  # If the person survives\n",
    "                            df.at[index, 'Infectious'] = 0\n",
    "                            if index in df_infectious.index:\n",
    "                                df_infectious.drop([index])\n",
    "                            df.at[index, 'Recovered'] = True  # Recover the person\n",
    "                    elif mt + 1 == moves_under_policy:\n",
    "                        if person['Infectious'] == 0.5:\n",
    "                            df.at[index, 'Infectious'] = 1\n",
    "                        else:\n",
    "                            df.at[index, 'Infectious'] = person['Infectious'] + 1  # Increase the infectious day counter\n",
    "                        \n",
    "                        \n",
    "                        # print(f'No. {index} infected {person.Infectious} in day {d} at {mt}')\n",
    "                                \n",
    "                \n",
    "                elif person['Exposed'] > 0:  # If a person is in exposed state\n",
    "                    \n",
    "                    if (person['Exposed'] - random.choice(range(0, 10))) >= Et:  # If the person has reached the exposed day limit?  7\n",
    "                        \n",
    "                        \n",
    "                        df.at[index, 'Exposed'] = 0\n",
    "                        df.at[index, 'Infectious'] = 0.5 if mt+1 != moves_under_policy else 1  # Increase the infectious day counter, now the person is infectious\n",
    "                        df_infectious.append(person)\n",
    "                    elif mt + 1 == moves_under_policy: # At the end of the day\n",
    "                        if person['Exposed'] == 0.5:\n",
    "                            df.at[index, 'Exposed'] = 1\n",
    "                        else:\n",
    "                            df.at[index, 'Exposed'] = person['Exposed'] + 1  # Increase the exposed day counter\n",
    "                        \n",
    "                        \n",
    "                elif person['Susceptible']:  # If the person is in susceptible state\n",
    "                    x_temp = int(person['x'])\n",
    "                    df_xtemp = df_infectious[['x']].to_numpy()\n",
    "                    if (x_temp in df_xtemp) or ((x_temp - 1) in df_xtemp) or ((x_temp + 1) in df_xtemp):\n",
    "                        y_temp = int(person['y'])\n",
    "                        df_ytemp = df_infectious[['y']].to_numpy()\n",
    "                        if (y_temp in df_ytemp) or ((y_temp - 1) in df_ytemp) or ((y_temp + 1) in df_ytemp):\n",
    "                            if random.choice(range(0, expose_rate)) > (expose_rate - 2):\n",
    "                                df.at[index, 'Exposed'] = 0.5 if mt+1 != moves_under_policy else 1\n",
    "                                df.at[index, 'Susceptible'] = False\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "def economy_gain(df):\n",
    "    economy_gain = len(df[(df.GG == False) & (df.Infectious == 0)]) * round(random.uniform(0.8,1), 2)\n",
    "    return economy_gain\n",
    "\n",
    "def current_state(df):\n",
    "    global economy\n",
    "    active_cases = len(df.loc[df['Infectious'] > 0])\n",
    "    new_inf = len(df.loc[df['Infectious'] == 1])\n",
    "    recovered = len(df.loc[df['Recovered'] == True])\n",
    "    gg = len(df.loc[df['GG'] == True])\n",
    "    reproduction_rate = len(df.loc[df['Infectious'] == 1]) / len(df.loc[df['Infectious'] > 1]) if len(df.loc[df['Infectious'] > 1]) > 0 else 0\n",
    "    economy = economy + economy_gain(df)\n",
    "        \n",
    "    return np.array([active_cases, new_inf, recovered, gg, reproduction_rate, economy])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "s = 200 #size of the grid\n",
    "N = 1000 #size of population\n",
    "M = round(N * 0.007) #Number of infectious population\n",
    "Et = 2 #Number of days staying exposed\n",
    "It = 21 #Number of days staying infectious\n",
    "Mt = 8 #Number of daily movements\n",
    "D = 100 #Number of days\n",
    "death_rate = 100\n",
    "expose_rate = 10\n",
    "\n",
    "#Initialization\n",
    "S = N - M #Susceptible population\n",
    "E = 0 #Exposed population\n",
    "I = M #Number of infectious population \n",
    "R = 0 #Recovered population\n",
    "P = S + E + I + R #Total population\n",
    "economy = 0 #Daily economic transaction\n",
    "\n",
    "policy_match = {0: 1, 1:0.75, 2:0.25} # assign action to policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "def_observation_space = Box(low = np.array([0,0,0,0,0,0]), high = np.array([P,P,P,P,1,P*D*Mt], dtype = int))\n",
    "##[active_cases, new_inf, recovered, gg, reproduction_rate, economy]\n",
    "# Create the virtual environment for RL\n",
    "class CoronaPolicy(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "        self.observation_space = def_observation_space # Box(low = 0, high = P, shape = (5,1), dtype = int )\n",
    "        # Dict(recovered=Discrete(P+1), sus=Discrete(P+1), exposed=Discrete(P+1),inf=Discrete(P+1),gg=Discrete(P+1))\n",
    "        \n",
    "        self.df = reset()\n",
    "        \n",
    "        self.state = np.array(current_state(self.df))\n",
    "        \n",
    "        self.day = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.df = one_day(self.df, action)\n",
    "        \n",
    "        self.state = current_state(self.df)\n",
    "        \n",
    "        self.day = self.day + 1\n",
    "        \n",
    "        reward = economy_gain(self.df)\n",
    "        \n",
    "        if self.day <= D:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "            \n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.observation_space = def_observation_space\n",
    "        \n",
    "        self.df = reset()\n",
    "        \n",
    "        self.state = current_state(self.df)\n",
    "        \n",
    "        self.day = 0\n",
    "        \n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CoronaPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation = 'relu',input_shape = states))\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(Dense(actions, activation = 'linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                224       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 2,531\n",
      "Trainable params: 2,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, GreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy() # ?\n",
    "    memory = SequentialMemory(limit = 1000, window_length = 1)\n",
    "    dqn = DQNAgent(model = model, memory = memory, policy = policy, \n",
    "                   nb_actions = actions, nb_steps_warmup = 10, target_model_update = 2000) # target_model_update\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   11/10000 [..............................] - ETA: 22:20:59 - reward: 866.0264"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\rl\\memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  925/10000 [=>............................] - ETA: 8:51:22 - reward: 708.9120"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr = 1e-3), metrics = ['mae'])\n",
    "dqn.fit(env, nb_steps = 2000, visualize = False, verbose = 1  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0: take action 1, total_reward: 1866.84. [[152.96213 154.71457 151.25412]]\n",
      "Day 1: take action 1, total_reward: 3703.8899999999994. [[388.22726 400.1671  392.26306]]\n",
      "Day 2: take action 1, total_reward: 5479.54. [[636.48315 655.56494 645.30273]]\n",
      "Day 3: take action 1, total_reward: 7191.2699999999995. [[887.29724 914.4976  897.8619 ]]\n",
      "Day 4: take action 1, total_reward: 8906.83. [[1133.044  1167.9805 1146.1385]]\n",
      "Day 5: take action 1, total_reward: 10627.07. [[1370.5613 1412.6481 1387.3358]]\n",
      "Day 6: take action 1, total_reward: 12388.849999999999. [[1591.2097 1639.5162 1612.8441]]\n",
      "Day 7: take action 1, total_reward: 14195.749999999998. [[1876.2662 1934.5146 1897.5265]]\n",
      "Day 8: take action 1, total_reward: 16005.159999999998. [[2161.5686 2228.5662 2181.8481]]\n",
      "Day 9: take action 1, total_reward: 17578.43. [[2421.5408 2495.3877 2442.5544]]\n",
      "Day 10: take action 1, total_reward: 19254.87. [[2779.9185 2851.3499 2788.36  ]]\n",
      "Day 11: take action 1, total_reward: 20908.45. [[3165.1592 3227.4407 3153.3225]]\n",
      "Day 12: take action 1, total_reward: 22406.08. [[3440.569  3508.0815 3426.2358]]\n",
      "Day 13: take action 1, total_reward: 23950.090000000004. [[3804.0796 3863.2559 3770.7075]]\n",
      "Day 14: take action 1, total_reward: 25256.210000000003. [[4167.4644 4217.653  4113.96  ]]\n",
      "Day 15: take action 1, total_reward: 26614.850000000002. [[4533.9185 4573.2163 4457.926 ]]\n",
      "Day 16: take action 1, total_reward: 27880.24. [[5010.758  5028.897  4898.2666]]\n",
      "Day 17: take action 1, total_reward: 28956.18. [[5401.41  5403.04  5261.245]]\n",
      "Day 18: take action 0, total_reward: 30008.22. [[5667.915  5663.929  5513.7764]]\n",
      "Day 19: take action 0, total_reward: 30798.09. [[6036.423  6013.1357 5852.6924]]\n",
      "Day 20: take action 0, total_reward: 31513.13. [[6498.062  6444.409  6262.4556]]\n",
      "Day 21: take action 0, total_reward: 32123.45. [[6748.8    6686.3257 6488.188 ]]\n",
      "Day 22: take action 0, total_reward: 32587.61. [[7143.6484 7055.661  6828.6914]]\n",
      "Day 23: take action 0, total_reward: 32950.84. [[7333.43   7237.911  6996.5044]]\n",
      "Day 24: take action 0, total_reward: 33230.549999999996. [[7566.3325 7456.528  7199.1157]]\n",
      "Day 25: take action 0, total_reward: 33479.229999999996. [[7502.3506 7403.6265 7150.9404]]\n",
      "Day 26: take action 0, total_reward: 33726.99999999999. [[7535.7866 7438.5664 7184.734 ]]\n",
      "Day 27: take action 0, total_reward: 33944.81999999999. [[7579.411  7482.3477 7227.232 ]]\n",
      "Day 28: take action 0, total_reward: 34137.799999999996. [[7676.297  7573.9873 7313.9526]]\n",
      "Day 29: take action 0, total_reward: 34344.31999999999. [[7648.7734 7552.377  7295.927 ]]\n",
      "Day 30: take action 0, total_reward: 34554.25999999999. [[7652.6006 7559.1533 7304.5566]]\n",
      "Day 31: take action 0, total_reward: 34782.359999999986. [[7611.5093 7524.838  7275.9688]]\n",
      "Day 32: take action 0, total_reward: 35047.499999999985. [[7522.492  7446.9443 7208.8843]]\n",
      "Day 33: take action 0, total_reward: 35325.83999999999. [[7438.79  7374.617 7147.915]]\n",
      "Day 34: take action 0, total_reward: 35660.51999999999. [[7272.246  7227.0283 7018.48  ]]\n",
      "Day 35: take action 0, total_reward: 36048.149999999994. [[7077.117  7056.3926 6859.214 ]]\n",
      "Day 36: take action 1, total_reward: 36546.19999999999. [[6827.245  6838.984  6648.7183]]\n",
      "Day 37: take action 1, total_reward: 37139.02999999999. [[6563.597 6611.868 6428.99 ]]\n",
      "Day 38: take action 1, total_reward: 37824.02999999999. [[6275.83   6363.749  6191.5474]]\n",
      "Day 39: take action 1, total_reward: 38657.98999999999. [[6009.4814 6136.719  5976.438 ]]\n",
      "Day 40: take action 1, total_reward: 39517.79999999999. [[5714.307  5878.588  5739.5947]]\n",
      "Day 41: take action 1, total_reward: 40490.459999999985. [[5445.0693 5576.5645 5520.437 ]]\n",
      "Day 42: take action 2, total_reward: 41603.79999999998. [[5226.6064 5310.8096 5330.476 ]]\n",
      "Day 43: take action 2, total_reward: 42721.65999999998. [[5181.6997 5270.8887 5298.9805]]\n",
      "Day 44: take action 2, total_reward: 43943.95999999998. [[5009.5435 5120.126  5149.5156]]\n",
      "Day 45: take action 2, total_reward: 45264.88999999998. [[4930.7734 5064.321  5072.5474]]\n",
      "Day 46: take action 1, total_reward: 46885.589999999975. [[4810.897  4950.043  4946.1387]]\n",
      "Day 47: take action 1, total_reward: 48594.68999999997. [[4793.86  4936.489 4917.013]]\n",
      "Day 48: take action 1, total_reward: 50129.23999999997. [[4894.7593 5042.055  5016.7344]]\n",
      "Day 49: take action 1, total_reward: 51683.259999999966. [[5018.7725 5171.0703 5142.9116]]\n",
      "Day 50: take action 1, total_reward: 53418.33999999997. [[5152.488  5310.218  5278.7715]]\n",
      "Day 51: take action 1, total_reward: 55191.81999999997. [[5324.796 5489.064 5456.116]]\n",
      "Day 52: take action 1, total_reward: 56771.43999999997. [[5497.727  5668.4146 5634.8022]]\n",
      "Day 53: take action 1, total_reward: 58464.729999999974. [[5640.292  5816.5786 5780.6   ]]\n",
      "Day 54: take action 1, total_reward: 60210.63999999997. [[5799.359  5982.0044 5942.728 ]]\n",
      "Day 55: take action 1, total_reward: 62018.81999999997. [[5972.2188 6160.888  6120.5615]]\n",
      "Day 56: take action 1, total_reward: 63834.66999999997. [[6185.422  6379.645  6342.4976]]\n",
      "Day 57: take action 1, total_reward: 65590.63999999997. [[6397.9077 6597.915  6563.2446]]\n",
      "Day 58: take action 1, total_reward: 67282.66999999997. [[6602.5    6808.101  6775.7495]]\n",
      "Day 59: take action 1, total_reward: 68946.72999999997. [[6813.591  7024.4463 6995.923 ]]\n",
      "Day 60: take action 1, total_reward: 70790.90999999996. [[7043.128  7259.6846 7235.356 ]]\n",
      "Day 61: take action 1, total_reward: 72458.26999999996. [[7272.9004 7495.165  7475.0356]]\n",
      "Day 62: take action 1, total_reward: 74076.85999999996. [[7479.4272 7706.836  7690.444 ]]\n",
      "Day 63: take action 1, total_reward: 75755.02999999996. [[7694.686  7927.3247 7915.191 ]]\n",
      "Day 64: take action 1, total_reward: 77681.44999999995. [[7928.2114 8166.5254 8159.0083]]\n",
      "Day 65: take action 1, total_reward: 79528.42999999996. [[8187.825 8432.449 8430.065]]\n",
      "Day 66: take action 2, total_reward: 81325.75999999997. [[8416.153  8666.317  8668.4375]]\n",
      "Day 67: take action 2, total_reward: 83113.15999999996. [[8668.243 8924.391 8931.361]]\n",
      "Day 68: take action 2, total_reward: 84880.69999999997. [[8889.234 9149.785 9160.199]]\n",
      "Day 69: take action 2, total_reward: 86826.97999999998. [[9135.232 9400.707 9413.652]]\n",
      "Day 70: take action 2, total_reward: 88634.23999999998. [[9393.002 9663.642 9679.237]]\n",
      "Day 71: take action 2, total_reward: 90421.63999999998. [[9606.287 9881.198 9898.987]]\n",
      "Day 72: take action 2, total_reward: 92149.45999999998. [[ 9856.209 10136.123 10156.484]]\n",
      "Day 73: take action 2, total_reward: 93897.13999999997. [[10073.418 10357.684 10380.279]]\n",
      "Day 74: take action 2, total_reward: 95664.67999999996. [[10308.944 10597.93  10622.946]]\n",
      "Day 75: take action 2, total_reward: 97541.44999999997. [[10552.325 10846.182 10873.704]]\n",
      "Day 76: take action 2, total_reward: 99318.91999999997. [[10799.629 11098.438 11128.504]]\n",
      "Day 77: take action 2, total_reward: 101225.47999999997. [[11033.85  11337.35  11369.825]]\n",
      "Day 78: take action 2, total_reward: 102913.57999999996. [[11269.377 11577.594 11612.492]]\n",
      "Day 79: take action 2, total_reward: 104790.34999999996. [[11508.828 11821.844 11859.204]]\n",
      "Day 80: take action 2, total_reward: 106567.81999999996. [[11754.824 12072.767 12112.657]]\n",
      "Day 81: take action 2, total_reward: 108414.79999999996. [[11978.603 12300.781 12343.186]]\n",
      "Day 82: take action 2, total_reward: 110122.75999999997. [[12218.336 12542.73  12589.84 ]]\n",
      "Day 83: take action 2, total_reward: 112049.17999999998. [[12456.76  12783.357 12835.146]]\n",
      "Day 84: take action 2, total_reward: 113906.08999999998. [[12705.664  13034.5625 13091.235 ]]\n",
      "Day 85: take action 2, total_reward: 115703.41999999998. [[12950.638 13281.798 13343.279]]\n",
      "Day 86: take action 2, total_reward: 117540.46999999999. [[13179.891 13513.171 13579.151]]\n",
      "Day 87: take action 2, total_reward: 119278.21999999999. [[13423.556 13759.085 13829.849]]\n",
      "Day 88: take action 2, total_reward: 121125.19999999998. [[13655.43  13993.102 14068.415]]\n",
      "Day 89: take action 2, total_reward: 122843.08999999998. [[13896.472 14236.37  14316.416]]\n",
      "Day 90: take action 2, total_reward: 124690.06999999999. [[14124.415 14466.421 14550.939]]\n",
      "Day 91: take action 2, total_reward: 126457.60999999999. [[14372.01  14716.302 14805.682]]\n",
      "Day 92: take action 2, total_reward: 128235.07999999999. [[14607.815 14954.286 15048.292]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 93: take action 2, total_reward: 130022.48. [[14849.539 15198.594 15296.567]]\n",
      "Day 94: take action 2, total_reward: 131909.18. [[15089.007 15442.162 15540.649]]\n",
      "Day 95: take action 2, total_reward: 133795.88. [[15344.439 15701.968 15801.003]]\n",
      "Day 96: take action 2, total_reward: 135533.63. [[15578.589 15940.127 16039.664]]\n",
      "Day 97: take action 2, total_reward: 137231.66. [[15808.743 16174.22  16274.252]]\n",
      "Day 98: take action 2, total_reward: 139138.22. [[16053.534 16423.201 16523.758]]\n",
      "Day 99: take action 2, total_reward: 140846.18000000002. [[16290.344 16664.064 16765.13 ]]\n",
      "Day 100: take action 2, total_reward: 142574.0. [[16523.16  16900.87  17002.434]]\n",
      "Day 101: take action 2, total_reward: 144242.24. [[16753.316 17134.963 17237.021]]\n",
      "Day 102: take action 2, total_reward: 146009.78. [[16990.123 17375.822 17478.389]]\n",
      "Day 103: take action 2, total_reward: 147717.74. [[17214.957 17604.51  17707.557]]\n",
      "Day 104: take action 2, total_reward: 149455.49. [[17437.129 17830.488 17934.012]]\n",
      "Day 105: take action 2, total_reward: 151242.88999999998. [[17675.268 18072.701 18176.736]]\n",
      "Day 106: take action 2, total_reward: 153089.86999999997. [[17926.71  18328.447 18433.023]]\n",
      "Day 107: take action 2, total_reward: 154986.49999999997. [[18179.482 18585.549 18690.664]]\n",
      "Day 108: take action 2, total_reward: 156595.15999999995. [[18412.3   18822.352 18927.969]]\n",
      "Day 109: take action 2, total_reward: 158481.85999999996. [[18647.777 19061.857 19167.982]]\n",
      "Day 110: take action 2, total_reward: 160289.11999999994. [[18885.918 19304.07  19410.709]]\n",
      "Day 111: take action 2, total_reward: 162155.95999999993. [[19141.352 19563.883 19671.064]]\n",
      "Day 112: take action 2, total_reward: 163943.35999999993. [[19372.838 19799.33  19907.012]]\n",
      "Day 113: take action 2, total_reward: 165611.59999999992. [[19620.285 20051.016 20159.227]]\n",
      "Day 114: take action 2, total_reward: 167418.85999999993. [[19858.426 20293.232 20401.955]]\n",
      "Day 115: take action 2, total_reward: 169285.69999999992. [[20099.223 20538.15  20647.395]]\n",
      "Day 116: take action 2, total_reward: 171023.44999999992. [[20338.693 20781.72  20891.475]]\n",
      "Day 117: take action 2, total_reward: 172790.9899999999. [[20576.832 21023.938 21134.2  ]]\n",
      "Day 118: take action 2, total_reward: 174637.96999999988. [[20818.959 21270.21  21380.998]]\n",
      "Day 119: take action 2, total_reward: 176465.08999999988. [[21059.76  21515.13  21626.434]]\n",
      "Day 120: take action 2, total_reward: 178123.39999999988. [[21293.906 21753.291 21865.094]]\n",
      "Day 121: take action 2, total_reward: 179900.86999999988. [[21512.092 21975.209 22087.479]]\n",
      "Day 122: take action 2, total_reward: 181747.8499999999. [[21772.844 22240.428 22353.256]]\n",
      "Day 123: take action 2, total_reward: 183455.80999999988. [[22008.322 22479.936 22593.27 ]]\n",
      "Day 124: take action 2, total_reward: 185292.8599999999. [[22250.45  22726.209 22840.062]]\n",
      "Day 125: take action 2, total_reward: 187139.83999999988. [[22493.908 22973.836 23088.213]]\n",
      "Day 126: take action 2, total_reward: 188947.0999999999. [[22724.062 23207.934 23322.805]]\n",
      "Day 127: take action 2, total_reward: 190615.33999999988. [[22967.53  23455.559 23570.953]]\n",
      "Day 128: take action 2, total_reward: 192372.9499999999. [[23187.04  23678.83  23794.695]]\n",
      "Day 129: take action 2, total_reward: 194200.0699999999. [[23429.168 23925.104 24041.488]]\n",
      "Day 130: take action 2, total_reward: 195937.8199999999. [[23665.977 24165.97  24282.863]]\n",
      "Day 131: take action 2, total_reward: 197645.77999999988. [[23902.783 24406.828 24524.229]]\n",
      "Day 132: take action 2, total_reward: 199423.24999999988. [[24147.574 24655.812 24773.738]]\n",
      "Day 133: take action 2, total_reward: 201141.13999999987. [[24365.756 24877.73  24996.125]]\n",
      "Day 134: take action 2, total_reward: 202868.95999999988. [[24603.893 25119.947 25238.848]]\n",
      "Day 135: take action 2, total_reward: 204715.9399999999. [[24839.375 25359.453 25478.865]]\n",
      "Day 136: take action 2, total_reward: 206483.47999999986. [[25090.814 25615.197 25735.146]]\n",
      "Day 137: take action 2, total_reward: 208151.71999999988. [[25312.988 25841.172 25961.604]]\n",
      "Day 138: take action 2, total_reward: 209829.88999999987. [[25540.479 26072.566 26193.482]]\n",
      "Day 139: take action 2, total_reward: 211716.58999999988. [[25782.615 26318.844 26440.277]]\n",
      "Day 140: take action 2, total_reward: 213643.00999999986. [[26034.055 26574.59  26696.564]]\n",
      "Day 141: take action 2, total_reward: 215301.31999999986. [[26269.533 26814.098 26936.576]]\n",
      "Day 142: take action 2, total_reward: 216949.69999999987. [[26491.705 27040.074 27163.03 ]]\n",
      "Day 143: take action 2, total_reward: 218786.74999999988. [[26721.861 27274.17  27397.625]]\n",
      "Day 144: take action 2, total_reward: 220663.5199999999. [[26982.617 27539.389 27663.396]]\n",
      "Day 145: take action 2, total_reward: 222411.19999999992. [[27212.773 27773.484 27897.99 ]]\n",
      "Day 146: take action 2, total_reward: 224148.94999999992. [[27445.59  28010.29  28135.293]]\n",
      "Day 147: take action 2, total_reward: 225886.69999999992. [[27697.031 28266.037 28391.578]]\n",
      "Day 148: take action 2, total_reward: 227614.51999999993. [[27915.21  28487.95  28613.963]]\n",
      "Day 149: take action 2, total_reward: 229431.70999999993. [[28166.654 28743.7   28870.25 ]]\n",
      "Day 150: take action 2, total_reward: 231189.31999999992. [[28406.123 28987.268 29114.332]]\n",
      "Day 151: take action 2, total_reward: 232996.5799999999. [[28632.357 29217.107 29344.816]]\n",
      "Day 152: take action 2, total_reward: 234793.9099999999. [[28869.53  29456.89  29585.969]]\n",
      "Day 153: take action 2, total_reward: 236650.8199999999. [[29116.031 29706.11  29836.61 ]]\n",
      "Day 154: take action 2, total_reward: 238448.1499999999. [[29362.531 29955.326 30087.252]]\n",
      "Day 155: take action 2, total_reward: 240394.4299999999. [[29619.688 30215.314 30348.73 ]]\n",
      "Day 156: take action 2, total_reward: 242291.0599999999. [[29879.514 30478.002 30612.914]]\n",
      "Day 157: take action 2, total_reward: 244018.87999999992. [[30124.684 30725.877 30862.201]]\n",
      "Day 158: take action 2, total_reward: 245806.2799999999. [[30347.197 30950.838 31088.457]]\n",
      "Day 159: take action 2, total_reward: 247653.2599999999. [[30599.025 31205.441 31344.512]]\n",
      "Day 160: take action 2, total_reward: 249400.93999999992. [[30837.531 31446.574 31587.023]]\n",
      "Day 161: take action 2, total_reward: 251049.31999999992. [[31069.379 31680.977 31822.764]]\n",
      "Day 162: take action 2, total_reward: 252757.2799999999. [[31299.89  31914.02  32057.143]]\n",
      "Day 163: take action 2, total_reward: 254604.25999999992. [[31534.395 32151.115 32295.588]]\n",
      "Day 164: take action 2, total_reward: 256361.8699999999. [[31767.572 32386.86  32532.678]]\n",
      "Day 165: take action 2, total_reward: 258119.4799999999. [[32015.402 32637.422 32784.676]]\n",
      "Day 166: take action 2, total_reward: 259837.36999999988. [[32240.586 32865.086 33013.645]]\n",
      "Day 167: take action 2, total_reward: 261634.6999999999. [[32472.426 33099.48  33249.375]]\n",
      "Day 168: take action 2, total_reward: 263402.2399999999. [[32724.258 33354.09  33505.434]]\n",
      "Day 169: take action 2, total_reward: 265249.2199999999. [[32953.438 33585.793 33738.465]]\n",
      "Day 170: take action 2, total_reward: 267056.47999999986. [[33210.594 33845.78  33999.938]]\n",
      "Day 171: take action 2, total_reward: 268843.87999999983. [[33441.11  34078.84  34234.324]]\n",
      "Day 172: take action 2, total_reward: 270581.62999999983. [[33688.934 34329.395 34486.32 ]]\n",
      "Day 173: take action 2, total_reward: 272349.1699999998. [[33915.453 34558.402 34716.633]]\n",
      "Day 174: take action 2, total_reward: 274186.2199999998. [[34159.816 34807.527 34965.695]]\n",
      "Day 175: take action 2, total_reward: 275784.94999999984. [[34387.617 35041.27  35198.32 ]]\n",
      "Day 176: take action 2, total_reward: 277552.4899999999. [[34602.18  35261.426 35417.426]]\n",
      "Day 177: take action 2, total_reward: 279290.2399999999. [[34855.145 35520.99  35675.75 ]]\n",
      "Day 178: take action 2, total_reward: 280998.1999999999. [[35082.95  35754.73  35908.383]]\n",
      "Day 179: take action 2, total_reward: 282646.5799999999. [[35296.176 35973.523 36126.13 ]]\n",
      "Day 180: take action 2, total_reward: 284513.41999999987. [[35533.254 36216.777 36368.227]]\n",
      "Day 181: take action 2, total_reward: 286251.16999999987. [[35778.27  36468.184 36618.434]]\n",
      "Day 182: take action 2, total_reward: 288108.0799999999. [[36011.37  36707.363 36856.477]]\n",
      "Day 183: take action 2, total_reward: 289696.87999999995. [[36244.47  36946.535 37094.508]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 184: take action 2, total_reward: 291603.44. [[36477.57  37185.723 37332.55 ]]\n",
      "Day 185: take action 2, total_reward: 293410.7. [[36731.86 37446.64 37592.23]]\n",
      "Day 186: take action 2, total_reward: 295168.31. [[36959.664 37680.387 37824.86 ]]\n",
      "Day 187: take action 2, total_reward: 296856.41000000003. [[37186.14  37912.766 38056.133]]\n",
      "Day 188: take action 2, total_reward: 298604.09. [[37431.156 38164.176 38306.34 ]]\n",
      "Day 189: take action 2, total_reward: 300272.33. [[37645.72 38384.33 38525.44]]\n",
      "Day 190: take action 2, total_reward: 302109.38. [[37892.06  38637.1   38777.008]]\n",
      "Day 191: take action 2, total_reward: 303777.62. [[38117.21  38868.117 39006.93 ]]\n",
      "Day 192: take action 2, total_reward: 305495.51. [[38338.395 39095.066 39232.797]]\n",
      "Day 193: take action 2, total_reward: 307203.47000000003. [[38575.46  39338.324 39474.895]]\n",
      "Day 194: take action 2, total_reward: 309000.80000000005. [[38816.508 39585.65  39721.043]]\n",
      "Day 195: take action 2, total_reward: 310827.92000000004. [[39046.96  39822.113 39956.38 ]]\n",
      "Day 196: take action 2, total_reward: 312704.69000000006. [[39291.98  40073.523 40206.586]]\n",
      "Day 197: take action 2, total_reward: 314412.6500000001. [[39529.055 40316.78  40448.68 ]]\n",
      "Day 198: take action 2, total_reward: 316209.9800000001. [[39764.797 40558.67  40689.42 ]]\n",
      "Day 199: take action 2, total_reward: 318037.1000000001. [[40001.87  40801.926 40931.52 ]]\n"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "import tensorflow as tf\n",
    "df = reset()\n",
    "economy = 0\n",
    "states = []\n",
    "for day in range(0, 200):\n",
    "    state = current_state(df)\n",
    "    states.append(state)\n",
    "    \n",
    "    state = tf.reshape(state, [1, 6])\n",
    "    \n",
    "    prediction = model.predict(state, steps = 1)\n",
    "    action_by_agent = np.argmax(prediction)\n",
    "    df = one_day(df, action = action_by_agent)\n",
    "    gain = economy_gain(df)\n",
    "    economy += gain\n",
    "    print(f\"Day {day}: take action {action_by_agent}, total_reward: {economy}. {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model_ann_4_6_states_BoltzmannQPolicy_tmu2000\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model_ann_4_6_states_BoltzmannQPolicy_tmu2000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
